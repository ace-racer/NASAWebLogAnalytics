---
title: "Sequence Mining Code - Host-based"
output: html_notebook
---

```{r}

#######################################################################
# the supporting functions
#######################################################################

#remove duplicate items from a basket (itemstrg)
uniqueitems <- function(itemstrg) {
  unique(as.list(strsplit(gsub(" ","",itemstrg),","))[[1]])
}

# execute ruleset using item as rule antecedent (handles single item antecedents only)
makepreds <- function(item, rulesDF) {
  antecedent = paste("<{",item,"}> =>",sep="") # NOTE: diff from assoc analysis same fn 
  firingrules = rulesDF[grep(antecedent, rulesDF$rule,fixed=TRUE),1] # rules is now rule
  #gsub(" ","",toString(sub(">}","",sub(".*=> <{","",firingrules))))
  gsub(" ", "", toString(sub('\\}>', '', sub(".*=> <\\{", "", firingrules))))
}

# count how many predictions are in the basket of items already seen by that user 
# Caution : refers to "baskets" as a global
checkpreds <- function(preds, baskID) {
  plist = preds[[1]]
  blist = baskets[baskets$sequenceID == baskID,"webpage"][[1]]
  cnt = 0 
  for (p in plist) {
    if (p %in% blist) cnt = cnt+1
  }
  cnt
}

# count all predictions made
countpreds <- function(predlist) {
  len = length(predlist)
  if (len > 0 && (predlist[[1]] == "")) 0 # avoid counting an empty list
  else len
}
```


```{r}
library(arulesSequences)
setwd("X:\\NASAWebLogAnalytics\\association_analysis")
data <- read_baskets(con = "nasa_data/sessionized_data/hostonly_july_ordered.txt", info = c("sequenceID","eventID","SIZE"))

```


```{r}
as(head(data), "data.frame") # view first few rows of the data
```


```{r}
seqs <- cspade(data, parameter = list(support = 0.05), control = list(verbose = TRUE))
as(seqs,"data.frame")  # view the sequences

```

```{r}
rules <- ruleInduction(seqs, confidence = 0.1,control = list(verbose = TRUE))
rulesDF <- as(rules,"data.frame")  # view the rules
rulesDF
```
```{r}
summary(rules)
```



```{r}
#read the test data
testegs = read.csv(file="nasa_data/sessionized_data/hostonly_aug_ordered.csv"); 
head(testegs)
```
```{r}
# which are the top rules by lift
top.lift <- sort(rules, decreasing = TRUE, na.last = NA, by = "lift")
inspect(head(top.lift, 20)) 
```
```{r}


```

```{r}
#execute rules against test data
testegs$preds = apply(testegs,1,function(X) makepreds(X["webpage"], rulesDF))
```


```{r}
# extract unique items bought (or rated highly) for each test user
baskets = as.data.frame(aggregate(webpage ~ sequenceID, data = testegs, paste, collapse=","))
baskets$webpage = apply(baskets,1,function(X) uniqueitems(X["webpage"]))
```

```{r}
# extract unique predictions for each test user
userpreds = as.data.frame(aggregate(preds ~ sequenceID, data = testegs, paste, collapse=","))
userpreds$preds = apply(userpreds,1,function(X) uniqueitems(X["preds"])) 
```

```{r}
#count how many unique predictions made are correct, i.e. have previously been bought (or rated highly) by the user
correctpreds = sum(apply(userpreds,1,function(X) checkpreds(X["preds"],X["sequenceID"])))
# count total number of unique predictions made
totalpreds = sum(apply(userpreds,1,function(X) countpreds(X["preds"][[1]]))) 
```

```{r}

precision = correctpreds*100/totalpreds

# Recall calculation
uniquepages <- testegs[c('sequenceID', 'webpage')]
actual_pages_visited <- table(uniquepages$sequenceID)
recall <- correctpreds*100/sum(actual_pages_visited) 

cat("precision=", precision, "corr=",correctpreds, "recall=",recall, "total=",totalpreds)
```