---
title: "NASA web logs association analysis"
output: html_notebook
---


```{r}
setwd("H:\\EB5205BarryWebAnalyticsCA\\nasa_source_code")
library("arules")
nasa_transactions = read.transactions(file="nasa_data/sessionized_data/sessionize_date_july.csv",rm.duplicates=TRUE, format="single", sep=",", cols=c("session_id","webpage"));
```
```{r}
rules <- apriori(nasa_transactions, parameter = list(supp=0.01, conf=0.01, minlen=2))
summary(rules)
#inspect(rules)
```
```{r}
# a useful plot of training data
itemFrequencyPlot(nasa_transactions,topN=20,type="absolute")
```
Interesting findings:
1. Apollo 13 the movie on the eventful 1969 mission to the moon was released at end of June 1995 which spiked a lot of interest in the Apollo 13 mission. Incidentally, Apollo 13 had launched from the KSC and so many people were searching for the history of the Apollo 13 mission presumably since the internet was in its nascent state and Wikipedia (or other open internet based encyclopedias) were not yet launched then.
References: 1. https://en.wikipedia.org/wiki/Apollo_13_(film)

2. STS 71 mission is also a top web page visited since it was launched towards the end of June 1995 from the KSC.






```{r}
#read the test data
setwd("H:\\EB5205BarryWebAnalyticsCA\\nasa_source_code")

nasa_aug_file = read.csv(file="nasa_data/sessionized_data/sessionize_date_aug.csv")
nasa_aug_file <- nasa_aug_file[c("session_id", "webpage")]
head(nasa_aug_file)
```
```{r}
# execute ruleset using item as rule antecedent (handles single item antecedents only)
makepreds <- function(item, rulesDF) {
  antecedent = paste("{",item,"} =>",sep="") 
  firingrules = rulesDF[grep(antecedent, rulesDF$rules,fixed=TRUE),1]
  gsub(" ","",toString(sub("\\}","",sub(".*=> \\{","",firingrules))))
}

rulesDF = as(rules,"data.frame")
nasa_aug_file$preds = apply(nasa_aug_file,1,function(X) makepreds(X["webpage"], rulesDF))
```

```{r}
head(nasa_aug_file)
```
```{r}
# which are the top rules by lift
top.lift <- sort(rules, decreasing = TRUE, na.last = NA, by = "lift")
inspect(head(top.lift, 20)) 
```
```{r}

# extract unique predictions for each test session - predictions
userpreds = as.data.frame(aggregate(preds ~ session_id, data = nasa_aug_file, paste, collapse=","))
userpreds$preds = apply(userpreds,1,function(X) uniqueitems(X["preds"]))

# extract unique items visited most for each test session - actual
baskets = as.data.frame(aggregate(webpage ~ session_id, data = nasa_aug_file, paste, collapse=","))
baskets$webpage = apply(baskets,1,function(X) uniqueitems(X["webpage"]))

baskets

```
```{r}
#remove duplicate items from a basket (itemstrg)
uniqueitems <- function(itemstrg) {
  unique(as.list(strsplit(gsub(" ","",itemstrg),","))[[1]])
}

# count how many predictions are in the basket of items already seen by that user 
# Caution : refers to "baskets" as a global
checkpreds <- function(preds, baskID) {
  # print(baskID)
  plist = preds[[1]]
  blist = baskets[baskets$session_id == baskID,"webpage"][[1]]
  cnt = 0 
  for (p in plist) {
    if (p %in% blist) cnt = cnt+1
  }
  cnt
}

# count all predictions made
countpreds <- function(predlist) {
  len = length(predlist)
  if (len > 0 && (predlist[[1]] == "")) 0 # avoid counting an empty list
  else len
}

```
```{r}
#count how many unique predictions made are correct, i.e. have previously been bought (or rated highly) by the user
correctpreds = sum(apply(userpreds,1,function(X) checkpreds(X["preds"],X["session_id"])))

# count total number of unique predictions made
totalpreds = sum(apply(userpreds,1,function(X) countpreds(X["preds"][[1]]))) 

precision = correctpreds*100/totalpreds

cat("precision=", precision, "corr=",correctpreds,"total=",totalpreds)

```

## precision= 11.11818 corr= 137804 total= 1239448

```{r}
library(arulesViz)
#plot(rules)
plotly_arules(rules)

```

```{r}
plot(rules, method="graph")
plot(rules, method="graph",nodeCol=grey.colors(10),edgeCol=grey(.7),alpha=1)
plot(rules, method="matrix")
plot(rules, method="paracoord", control=list(reorder=TRUE))
```


```{r}
total_relevant_instances = nrow(nasa_aug_file)
recall = correctpreds * 100 / total_relevant_instances
recall
```

## Recall comes to 33.11 % 

```{r}
library(arulesViz)
plot(top.lift, method="graph",nodeCol=grey.colors(10),edgeCol=grey(.7),alpha=1)

```